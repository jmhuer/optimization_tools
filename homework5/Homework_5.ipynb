{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework 5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jmhuer/optimization_tools/blob/main/homework5/Homework_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpTRNuz3KDWV"
      },
      "source": [
        "# Problem Set 5\n",
        "In this problem set you will implement the Conjugate Gradient algorithm, also known as the Frank Wolfe algorithm, and you will compare it to algiorithms you have implemented on previous homeworks. You will also play with Mirror Descent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aaF7Pysb1kIY",
        "outputId": "4d779c4e-da5e-4a5c-eb11-9036eabc98a1"
      },
      "source": [
        "# ! git clone https://github.com/jmhuer/optimization_tools\n",
        "import optimization_tools.utils \n",
        "\n",
        "\n",
        "import re\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "a = 'https://drive.google.com/file/d/1YDmMRdgRRJSKaWGR7c0g6mzj73Bzf9MU/view?usp=sharing'\n",
        "b ='https://drive.google.com/file/d/1ZdnrCU7YotTAU0duRHTvUUF-k23lG7Wd/view?usp=sharing'\n",
        "lg_news = 'https://drive.google.com/file/d/1mjADAoaHTISJYFuUzhWU_LkFFtdrw3ZF/view?usp=sharing'\n",
        "\n",
        "def download_gdrive(share_link, local_name, print_stout=False):\n",
        "  id = re.search('d/(.*?)/view', share_link).group(1)\n",
        "  command = \"wget --load-cookies /tmp/cookies.txt \\\"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate \\'https://docs.google.com/uc?export=download&id={}\\' -O- | sed -rn \\'s/.*confirm=([0-9A-Za-z_]+).*/\\\\1\\\\n/p')&id={}\\\"  -O \\'{}\\' && rm -rf /tmp/cookies.txt\".format(id,id,local_name)\n",
        "  returned_value = subprocess.run(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT) \n",
        "  if print_stout: print(returned_value.stdout.decode(\"utf-8\"))\n",
        "  else: print(\"Download Complete: \" + local_name) \n",
        "\n",
        "download_gdrive(a, \"a.npy\")\n",
        "download_gdrive(b, \"b.npy\")\n",
        "download_gdrive(lg_news, \"logistic_news.zip\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'optimization_tools'...\n",
            "remote: Enumerating objects: 20, done.\u001b[K\n",
            "remote: Counting objects: 100% (20/20), done.\u001b[K\n",
            "remote: Compressing objects: 100% (16/16), done.\u001b[K\n",
            "remote: Total 20 (delta 1), reused 20 (delta 1), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (20/20), done.\n",
            "Download Complete: a.npy\n",
            "Download Complete: b.npy\n",
            "Download Complete: logistic_news.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Tr_l4PW3vzy"
      },
      "source": [
        "# %rm -rf /content/optimization_tools"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YSu7lbrKlmJ"
      },
      "source": [
        "# Problem 1: LASSO via Frank Wolfe\n",
        "Recall the least squares problem with $\\ell^1$ regularization from the previous two homeworks:\n",
        "$$\n",
        "\\min_x \\left[f(x) = \\frac{1}{2}\\|{Ax-b}\\|_2^2 + \\lambda \\|{x}\\|_1 \\right]\n",
        "$$\n",
        "\n",
        "So far you have used the subgradient method, ISTA and FISTA (proxima gradient, and accelerated proximal gradient) to solve this problem. Now you will use Frank-Wolfe.\n",
        "\n",
        "As we cover in the lectures, the Frank-Wolfe (or conditional gradient) algorithm minimizes a smooth function $f(x)$ subject to a convex constraint $x \\in \\mathcal{X}$.\n",
        "\n",
        "When it is easy to minimize a linear function over $\\mathcal{X}$, Frank-Wolfe (FW) has several advantages including that it often produces sparse iterates and does not require a projection step to stay within $\\mathcal{X}$. In order to apply FW to the LASSO problem above, we can reformulate the problem as \n",
        "\n",
        "\\begin{eqnarray*}\n",
        "\\min_{{x}}: && \\frac{1}{2}\\|A{x}-{b}\\|_2^2, \\\\\n",
        "\\mbox{s.t. } && \\|{x}\\|_1 \\leq \\gamma.\n",
        "\\end{eqnarray*}\n",
        "\n",
        "This formulation is equivalent to the original LASSO formulation for a suitable value of $\\gamma$ (and depending on the $\\lambda$ in the original LASSO formulation). \n",
        "\n",
        "Run the FW algorithm for $10^4$ steps and find a $\\gamma$ that performs well empirically (you might consider the $\\ell^1$ norm of your solution from previous homework. Compare the results to sub-gradient method implemented in the previous homework. Again, separately record the (unsquared) error $\\|A{x}_t-{b}\\|$ and the regularization term $\\|{x}\\|_1$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joauE77UvivS"
      },
      "source": [
        "import numpy as np\r\n",
        "import numpy.random as rn\r\n",
        "import numpy.linalg as la\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import time\r\n",
        "\r\n",
        "A = np.load(\"A.npy\")\r\n",
        "b = np.load(\"b.npy\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3kjFQ3DmZUi"
      },
      "source": [
        "#Problem 2: Robust Regression, PSGD and Mirror Descent\n",
        "Consider the problem of robust regression, where some small number of measurements have been potentially completely corrupted. One way to formulate an optimization problem to solve this robust regression is as follows:\n",
        "\n",
        "\\begin{eqnarray*}\n",
        "\\min_{\\beta}: && \\|X \\beta - \\mathbf{y}\\|_1 \\\\\n",
        "{\\rm s.t.}: && \\beta \\in \\mathcal{X}.\n",
        "\\end{eqnarray*}\n",
        "\n",
        "The rationale for this formulation stems from the idea that because of the $\\ell^1$-error, huge errors are not disproportionally penalized, as they would be in the squared error formulation (this is the formulation we have worked with before, including in the previous problem), and therefore the optimal solution is less sensitive to outliers. You will solve this problem using Projected Subgradient Descent, and also Mirror Descent. Let the constraint set be the simplex: \n",
        "$$\n",
        "\\mathfrak{X} = \\{\\beta \\,:\\, \\beta \\geq 0, \\, \\sum \\beta_i = 1\\}.\n",
        "$$\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0ejMPxXtFKd"
      },
      "source": [
        "##Part (A)\r\n",
        "Write down the update for projected gradient descent. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6aQitVItFoB"
      },
      "source": [
        "##Part (B) \r\n",
        "Write down the mirror descent update. For this, we will use the mirror map $\\Phi(\\beta) = \\sum \\beta_i \\log \\beta_i$. Compute the Bregman divergence, $D_{\\phi}(\\mathbf{\\beta},\\mathbf{z})$ explicitly. <br>\r\n",
        "(We use $\\mathbf{z}$ here to not confuse this variable with $\\mathbf{y}$ in the robust regression problem above. $\\mathbf{z}$ is the intermediate term used in Mirror Descent, which was commonly denoted as $\\mathbf{y}$ in the lectures.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RDzOf6ptGHU"
      },
      "source": [
        "##Part (C)\r\n",
        "Using the data in X.npy and y.npy, and using stepsizes of your choosing, compare the projected subgradient method with mirror descent. What is $\\beta$? Plot the objective above against iterations for both methods (in a single plot). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NKIDiSnwqSB"
      },
      "source": [
        "X = np.load(\"X.npy\")\r\n",
        "y = np.load(\"y.npy\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3pUSorpwsqS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}